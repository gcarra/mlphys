{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as apd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import dataset_info, dataset_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predicitons: Linear Regression \n",
    "---\n",
    "Classification isn't the only task within the scope of supervised machine learning. **Regression** is another equally important task with many complemntary features. In regression, we are asked to make a real-valued prediction based upon a dataset of samples. As in classification, we can think of our dataset as an $N\\times P$ matrix, with $N$ samples and $P$ features. However, for supervised tasks, instead of binary/multi-valued *labels* corresponding to partitions of the dataset, we are, often, interested in matching real-valued *response* variables. \n",
    "\n",
    "In the case of simple linear regression, the problems we look at are of the form\n",
    "$$\\mathbf{y} = \\mathbf{X}\\mathbf{b} + \\mathbf{w},$$\n",
    "where $\\mathbf{y}$ are our response variables, the matrix $\\mathbf{X}$ is our dataset, the vector $\\mathbf{w}$ represents some noise or uncertainty on our model, and we hope to find the *explanatory* variables $\\mathbf{b}$ which are able to match our observations and which generalize to future observations. Additionally, one can generalize to a wide class of so-called genearlized linear models via\n",
    "$$\\mathbf{b} = {\\rm F}(\\mathbf{X}\\mathbf{b}),$$\n",
    "where $F(\\cdot)$ is often some stochastic function.\n",
    "\n",
    "Here, we will consider solving regressions via **maximum a posteriori (MAP)** approaches. From the Bayesian perspective we are often attempting to solve the problem\n",
    "$${\\rm arg~min}\\quad -\\log P(\\mathbf{y}|\\mathbf{X},\\mathbf{b}) - \\log P(\\mathbf{x}).$$ \n",
    "If we return to our first definition of linear regression, with additive noise that is assumed to be *iid* Gaussian, and assuming a uniform prior on the value of $\\mathbf{x}$, we arrive at the common least squares (LSQ) problem,\n",
    "$${\\rm arg~min}_{\\mathbf{b}} \\quad ||\\mathbf{y} - \\mathbf{X}\\mathbf{b} ||_2^2,$$\n",
    "which we can interpret as a **maximum likelihood (MaxLike)** solution.\n",
    "\n",
    "Regressions, and specifically linear regressions, often form the basis of the ___\"Do The Simplest Thing That Could Possibly Work.\"___ Many times in production, simple regressions provide good-enough performance which maximize the tradeoff against engineering costs (coding time, compute time). And, as we will later see, regressions can even be applied to solve classification tasks, as in *logistic* regression. So, truly, regressions should be the first stop when encountering a new data-science or ML problem. Things can only, hopefully, go up from here !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example\n",
    "---\n",
    "For example, lets think of the very simplest problem we can. How about a univariate problem,  \n",
    "$$ y_i = mx_i + (w_i\\sim\\mathcal{N}(0,\\Delta)),$$\n",
    "where the value of $x$ are our known data points, we observe some response variables $y$, and wish to estimate the scalar value of $m$ which relates the two. To keep this problem from being entirely trivial, we introduce some noise with variance $\\Delta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##--- Problem Parameters ---##\n",
    "m = 2            # Model parmaeter to be learned\n",
    "n = 10          # Number of samples to train with\n",
    "delta = 1e0     # Observational noise variance\n",
    "\n",
    "##--- Generate Data ---##\n",
    "x = 2*np.random.rand(n) - 1\n",
    "w = np.sqrt(delta)*np.random.randn(n)\n",
    "y = m*x + w\n",
    "\n",
    "##--- Visualize ---##\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(x,y,'ob')\n",
    "plt.xlabel(\"Datum\",fontsize = 16)\n",
    "plt.ylabel(\"Response Variable\", fontsize = 16)\n",
    "plt.axis([-5, 5, -5, 5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how can we estimate the response variable $m$ ? In this case it should be quite easy to find the right value of $m$, especially when we have a lot of data and the value of $\\Delta$ is not too high. Lets make a calculation on paper of what this value should be in a Bayesian sense. First we write the likelihood,\n",
    "$$ P(\\mathbf{y} | \\mathbf{X}, \\mathbf{b}) = \\frac{1}{\\sqrt{2\\pi \\Delta}} \\exp\\left[-\\frac{(\\mathbf{y} - m\\mathbf{x})^2}{2\\Delta}\\right].$$\n",
    "Now, for MAP estimation we want to look at the negative log value and we can drop any constant terms...\n",
    "\n",
    "$$-\\log P(\\mathbf{y}| \\mathbf{X}, \\mathbf{b}) \\propto (\\mathbf{y} - m\\mathbf{x})^2 $$\n",
    "$$-\\log P(\\mathbf{y}| \\mathbf{X}, \\mathbf{b}) \\propto \\mathbf{y}^T\\mathbf{y} + m^2\\mathbf{x}^T\\mathbf{x} - 2m\\mathbf{y}^T\\mathbf{x}$$\n",
    "\n",
    "Now we have the problem\n",
    "$${\\rm arg~min}_m \\quad -\\log P(\\mathbf{y}|\\mathbf{X}, \\mathbf{b})$$\n",
    "$${\\rm arg~min}_m \\quad \\mathbf{y}^T\\mathbf{y} + m^2\\mathbf{x}^T\\mathbf{x} -\n",
    "2m\\mathbf{y}^T\\mathbf{x}$$\n",
    "$${\\rm arg~min}_m \\quad m^2\\mathbf{x}^T\\mathbf{x} - 2m\\mathbf{y}^T\\mathbf{x}$$\n",
    "\n",
    "Since this is a convex problem over $m$, taking the minimizer is as simple as finding the root of the derivative, hence,\n",
    "$$m^* = {\\rm arg~min}_m \\quad m^2\\mathbf{x}^T\\mathbf{x} - 2m\\mathbf{y}^T\\mathbf{x} = \\frac{\\mathbf{y}^T\\mathbf{x}}{\\mathbf{x^T}\\mathbf{x}}$$\n",
    "\n",
    "---\n",
    "### Wakeup Task: Calculate the MAP $m^*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##--- Task 1 ---#\n",
    "# Calculate the MAP estimate of `m` given the dataset `x` given observed `y`.\n",
    "m_estimate = \n",
    "print(\"m* (MaxLike): \", m_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load task1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Wakeup-Task: Brute-Force Search\n",
    "\n",
    "*Step 1: Define Likelihood*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def likelihood(x, y, m, d):\n",
    "    \"\"\"\n",
    "        Define the likelihood for a given set of data and expanatory variable $m$,\n",
    "            y = mx + noise (iid Gaussian with variance d)\n",
    "    \"\"\"\n",
    "    p = \n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load task2-1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 2: Find all values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##--- Initialize Parameters ---##\n",
    "m_test_vals = np.linspace(-5,5,1000)\n",
    "like_over_m = np.zeros(len(m_test_vals))\n",
    "\n",
    "##--- Loop over Values ---##\n",
    "for (i,m_test) in enumerate(m_test_vals):\n",
    "    like_over_m[i] = likelihood(x,y,m_test,delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 3: Plot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##--- Plot Context ---##\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "##--- Plot Command ---##\n",
    "plt.plot(m_test_vals,like_over_m, label = \"Likelihood\")\n",
    "\n",
    "##--- Plot Formatting ---##\n",
    "plt.xlim([-5, 5])\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Value of $m$\",fontsize = 16)\n",
    "plt.ylabel(\"$P(y|F,x,m,\\Delta)$\", fontsize = 16)\n",
    "plt.axvline(m_estimate, color = \"k\", linestyle = \":\", label = \"m (MaxLike)\")\n",
    "plt.axvline(m, color = \"k\", linestyle = \"-\", label = \"m (True)\", linewidth = 0.5)\n",
    "plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##--- Plot Context ---##\n",
    "plt.figure(figsize=(7,7))\n",
    "\n",
    "##--- Plot Command ---##\n",
    "plt.plot(np.linspace(-5,5,100),m_estimate*np.linspace(-5,5,100),\":k\", linewidth=0.7, label = \"m (MaxLike)\")\n",
    "plt.plot(np.linspace(-5,5,100),m*np.linspace(-5,5,100),\"-k\", linewidth=0.7, label = \"m (True)\")\n",
    "plt.plot(x,y, \"ob\", label = \"Data\")\n",
    "\n",
    "##--- Plot Formatting ---##\n",
    "plt.axis([-5, 5, -5, 5])\n",
    "plt.xlabel(\"Datum\",fontsize = 16)\n",
    "plt.ylabel(\"Response\", fontsize = 16)\n",
    "plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "So we can see that even for this simple test, when the number of samples is low, we can mismatch the correct value of the slope, even for a quite easy problem like this. \n",
    "\n",
    "## Multiple Features\n",
    "---\n",
    "Lets take a look at some models with more than one feature. How can we solve these problems? In the context of linear regression, our features might be linear or non-linear, however the problem remains linear in terms of the predictors. For example, one could attempt to fit a polynomial model, \n",
    "$$ y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_P x^P,$$\n",
    "using a linear regression, e.g.\n",
    "$$ \\mathbf{y} = [\\mathbf{1}, \\mathbf{x}, \\mathbf{x}^2, \\cdots, \\mathbf{x}^P] \\times \\boldsymbol{\\beta} = \\mathbf{X}\\boldsymbol{\\beta}.$$\n",
    "\n",
    "A simple least squares can be used to fit such a model. Lets take a look at an example where the underlying model is an order-2 polynomial (quadratic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##--- Problem Parameters ---##\n",
    "a = 0.1         # Order-1 Coefficient\n",
    "b = -2           # Order-2 Coefficient\n",
    "C = -30         # Offset\n",
    "n = 100         # Number of samples to train with\n",
    "delta = 1e0     # Observational noise variance\n",
    "\n",
    "##--- Generate Data ---##\n",
    "x = 6*np.random.rand(n) - 3\n",
    "w = np.sqrt(delta)*np.random.randn(n)\n",
    "y = a*x + b*np.power(x,2) + C + w\n",
    "\n",
    "##--- Visualize ---##\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(x,y,'ob')\n",
    "plt.xlabel(\"Datum\",fontsize = 16)\n",
    "plt.ylabel(\"Response Variable\", fontsize = 16)\n",
    "# plt.axis([-5, 5, -5, 5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to solve this problem, we need to build the matrix of features. Since we are interested in polynomial fitting, lets write a function which builds the feature matrix for us.\n",
    "\n",
    "### Task: Create Power Features\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def power_features(x, max_power):\n",
    "    \"\"\"\n",
    "        Given a vector of data points, x, build a matrix of power\n",
    "        features from 0 (constant) up to power p for use with\n",
    "        polynomial fitting.\n",
    "    \"\"\"\n",
    "    #--- Initialize Data Matrix ---#\n",
    "    X = np.zeros((x.shape[0], max_power+1))\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load task3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets make a fit. So, how do we solve for all of the coefficients at once? We can follow through the same as in the simple slope regression example and arrive at the solution for the MaxLike estimate. Another way of looking at this is to see the problem through the lens of linear algebra, we want to find an estimate which minimizes the _residual sum of squares_,\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\beta}} = {\\rm arg~min}_{\\boldsymbol{\\beta}}\\quad{\\rm RSS}(\\boldsymbol{\\beta}) \\\\\n",
    "\\hat{\\boldsymbol{\\beta}} = {\\rm arg~min}_{\\boldsymbol{\\beta}}\\quad ||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}||_2^2 $$\n",
    "\n",
    "Finding the minimum of this convex cost is simply finding the zero of the gradient, as before,\n",
    "$$ \\frac{\\partial{\\rm RSS}}{\\partial\\boldsymbol{\\beta}} =\n",
    "-2\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}),$$\n",
    "\n",
    "$$\\therefore\\quad\n",
    "   \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\n",
    "   \\mathbf{X}^T\\mathbf{y},\n",
    "$$\n",
    "which gives us the classical LSQ solution (as presented in lecture). \n",
    "\n",
    "Now, let us take a look at fitting higher dimensional models to our dataset above. First, we just need to create the different feature sets for the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#--- Create Different Models ---#\n",
    "XQuad = power_features(x,2)\n",
    "XCube = power_features(x,3)\n",
    "XSilly = power_features(x,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#--- LSQ long-way using linalg.solve ---#\n",
    "# We can use np.linalg.solve for solving square systems...\n",
    "fitQuad = np.linalg.solve(np.dot(XQuad.T,XQuad),np.dot(XQuad.T,y))\n",
    "fitCube = np.linalg.solve(np.dot(XCube.T,XCube),np.dot(XCube.T,y))\n",
    "fitSilly = np.linalg.solve(np.dot(XSilly.T,XSilly),np.dot(XSilly.T,y))\n",
    "\n",
    "#--- LSQ directly using linalg.lstsq ---#\n",
    "# We can also use the explicit least squares function which does some\n",
    "# of this leg-work for us. Also returns a number of other items, see\n",
    "# documentation for all of it. (Fit parameters are given in the first\n",
    "# location of the returned tuple, hence the suffic `[0]`).\n",
    "fitQuad = np.linalg.lstsq(XQuad,y)[0]\n",
    "fitCube = np.linalg.lstsq(XCube,y)[0]\n",
    "fitSilly = np.linalg.lstsq(XSilly,y)[0]\n",
    "\n",
    "#--- Show fit params ---#\n",
    "print(\"Fit Quad. Model: \", fitQuad)\n",
    "print(\"Fit Cube. Model: \", fitCube)\n",
    "print(\"Fit Silly Model: \", fitSilly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, visualizing what these different models are on our dataset, we see the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#--- Create fit curve domain ---#\n",
    "xdom = np.linspace(-5,5,1000)\n",
    "Xdom = power_features(xdom,10)\n",
    "\n",
    "#--- Predictions of different models ---#\n",
    "yQuad = np.dot(Xdom[:,0:3], fitQuad)\n",
    "yCube = np.dot(Xdom[:,0:4], fitCube)\n",
    "ySilly = np.dot(Xdom, fitSilly)\n",
    "\n",
    "#--- Visualize ---#\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.plot(x,y,'ob',label = 'Data')\n",
    "plt.plot(xdom, yQuad,':k', label = 'P = 2')\n",
    "plt.plot(xdom, yCube,'-.k', label = 'P = 3')\n",
    "plt.plot(xdom, ySilly,'--k', label = 'P = 10')\n",
    "plt.plot(xdom,a*xdom + b*np.power(xdom,2) + C,'-k', linewidth = 0.5, label = 'True Model')\n",
    "plt.xlabel(\"Datum\",fontsize = 16)\n",
    "plt.ylabel(\"Response Variable\", fontsize = 16)\n",
    "plt.axis([-5,5,-100,10])\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see that within the range of the data points, with enough presented data, all of the models do well to fit the data. However, outside of those bounds, the error can be much larger. This is the problem of fitting versus generalization. In our case, the error agains the datapoints might be good for higher degree models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingRSSQuad = np.sum(np.power(y - np.dot(XQuad,fitQuad),2))\n",
    "trainingRSSCube = np.sum(np.power(y - np.dot(XCube,fitCube),2))\n",
    "trainingRSSSilly = np.sum(np.power(y - np.dot(XSilly,fitSilly),2))\n",
    "\n",
    "print(\"Trained RSS (Quad): \", trainingRSSQuad)\n",
    "print(\"Trained RSS (Cube): \", trainingRSSCube)\n",
    "print(\"Trained RSS (Silly): \", trainingRSSSilly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see that the crazy-high degree model does better at matching our training data ! But we can visually see that this model is far too powerful for the amount of data we have. It is able to fit the data while doing the wrong thing in general, as we can see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ytest = np.dot(Xdom[:,0:3],[C,a,b])\n",
    "testRSSQuad = np.sum(np.power(ytest - np.dot(Xdom[:,0:3],fitQuad),2))\n",
    "testRSSCube = np.sum(np.power(ytest - np.dot(Xdom[:,0:4],fitCube),2))\n",
    "testRSSSilly = np.sum(np.power(ytest - np.dot(Xdom,fitSilly),2))\n",
    "\n",
    "print(\"Trained RSS (Quad): %0.5g\" %testRSSQuad)\n",
    "print(\"Trained RSS (Cube): %0.5g\" %testRSSCube)\n",
    "print(\"Trained RSS (Silly): %0.5g\" % testRSSSilly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, a huge error when tested over a larger range of possible values ! This shows that we, in some way, need to make sure that we use the simplest model possible that generalizes. However, in practice, we often don't know how simple or complex the underlying data is. There are a couple of approaches here. One is to simply acquire more data, and make sure that you acquire data over the whole range of possible values. But, scaling up data acquisition might not be feasible for some applications. How can we tune the parameters that we use ? Lets take a look at **regularization**. \n",
    "\n",
    "## Regularization\n",
    "---\n",
    "In the examples we've looked at, we had a simple univariate problem and we were interested in making regressions according to functions (polynomials) of that cure datum. In this case, we have some discretion in choosing how complicated of a model to pick. And we have a natural way to scale the generalization capability of the model.\n",
    "\n",
    "However, in practice we might not have such a convenience. Often in regression tasks we have some response variable, perhaps a patient diagnosis or an experimental result, and we have many, many features which were measured in parallel with that observation. We would like to estabilish some model of the result given some combination of these measured features. \n",
    "\n",
    "In this case, *a priori*, we have no idea which possible features we measure will be the best predictors. Our idea is simply to acquire as much data as we can and write an algorithm to untagle the correlations in the dataset. In this case, we need to have some method of *selecting* the best possible features. Additionally, we are often taking measurements of different features which might be highly correlated, thus reducing the overall predictiveness of our regression model (with respect to the number of measured features).\n",
    "\n",
    "Regularization offers us a possiblity of accounting for some of these obstacles. We want to admit as complicated of a model as possible (to drive down fitting error), but we want some degree of regularization in order to also promote the *simplest* model as possible. Lets take a look at a few approaches.\n",
    "\n",
    "### Ridge Regression\n",
    "In ridge regression (RR), we want to fit our response variables, but we want the model parameters $\\beta$ to have a small variance. We can accomplish this via an $\\ell_2$ penalty on the paramters,\n",
    "$$ \\hat{\\boldsymbol{\\beta}} = {\\rm arg~min}_{\\boldsymbol{\\beta}}\\quad\n",
    "    ||\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}||_2^2 + \\lambda || \\boldsymbol{\\beta}||_2^2,$$\n",
    "In this regression, the regularization term $\\lambda$ serves as a tuning term which balances between the model fit and the variance of the model parameters. Because this regularization penalizes \"long\" parameter vectors via the $\\ell_2$ norm, this kind of regression is often referred to as a **shrinkage**. \n",
    "\n",
    "There is a direct analytical solution to this regression, as in the case of least squares. We just need to solve\n",
    "$$ \\hat{\\boldsymbol{\\beta}} = \n",
    "    (\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}.\n",
    "$$\n",
    "\n",
    "As a test case, lets look at a random problem from high-dimensional statistics. Often we are confronted with *many* features, but *few* measurements. However, we also often know that of the many features we measure, potentially very few of them contribute significantly to our response variables.\n",
    "\n",
    "In general, we cannot use an OLS estimator to solve such under-determined problems. There exists an entire null-space of potential solutions, and we have no information to guide us in how to make a unique solution. However, by using regression, we are enforcing a scoring to this space of solutions, allowing us to choose a \"best\" solution, up to how we define the regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#--- Experiment Parameters ---#\n",
    "n = 50           # Number of Samples\n",
    "p = 100           # Number of Parameters\n",
    "delta = 0.1    # Model error variance\n",
    "lam = 0.1        # Regularization level\n",
    "\n",
    "#--- Random Dataset ---#\n",
    "X = np.random.randn(n,p)/np.sqrt(p)\n",
    "\n",
    "# Create ground-truth model parameters\n",
    "betaModel = np.zeros(p)\n",
    "betaModel[15] = 3\n",
    "betaModel[30] = -6\n",
    "# Form observations\n",
    "y = np.dot(X, betaModel) + np.sqrt(delta)*np.random.randn(n)\n",
    "\n",
    "#--- Ridge Regression Solution ---#\n",
    "R = np.diag(np.full(p,lam))\n",
    "betaRR = np.linalg.solve(np.dot(X.T,X) + R,np.dot(X.T,y))\n",
    "\n",
    "#--- Visualization ---#\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.stem(betaRR, label='Ridge $\\\\hat{\\\\beta}$')\n",
    "plt.plot(np.argwhere(betaModel),betaModel[betaModel != 0],'kx', label='True $\\\\beta$')\n",
    "plt.axis([0, p-1, -7, 7])\n",
    "plt.xlabel('Features', fontsize=16)\n",
    "plt.ylabel('Coefficient Value', fontsize=16)\n",
    "plt.legend(loc=4, fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see that voila, we have a solution ! It isn't so close, but we can see that indeed we are able to see some correlation with the true model features. A question here is, what happens as we move the value of $\\lambda$. One common way of looking at the effect of the regularization term on the estimator is through the **L-Curve**. In this plot we look at the balance between the solution norm and the residual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldom = np.logspace(-5,2,100)\n",
    "\n",
    "rss = np.zeros(len(ldom))\n",
    "reg = np.zeros(len(ldom))\n",
    "mse = np.zeros(len(ldom))\n",
    "d1  = np.zeros(len(ldom))\n",
    "d2  = np.zeros(len(ldom))\n",
    "\n",
    "for (i,l) in enumerate(ldom):\n",
    "    R = np.diag(np.full(p,l))\n",
    "    fit = np.linalg.solve(np.dot(X.T,X) + R,np.dot(X.T,y))\n",
    "    rss[i] = np.mean(np.power(y - np.dot(X,fit),2))\n",
    "    reg[i] = np.mean(np.power(fit,2))\n",
    "    mse[i] = np.mean(np.power(betaModel - fit,2))\n",
    "    d1[i] = fit[15]\n",
    "    d2[i] = fit[30]\n",
    "    \n",
    "bestLambda = ldom[np.argmin(mse)] \n",
    "bestRSS = rss[np.argmin(mse)]\n",
    "bestReg = reg[np.argmin(mse)]\n",
    "    \n",
    "#--- Visualize ---#\n",
    "plt.figure(figsize=(15,5))    \n",
    "plt.subplot(131)\n",
    "plt.loglog(rss,reg)\n",
    "plt.plot(rss[50],reg[50],'o',label='$\\\\lambda = %0.1e$' % ldom[50])\n",
    "plt.plot(rss[60],reg[60],'o',label='$\\\\lambda = %0.1e$' % ldom[60])\n",
    "plt.plot(rss[70],reg[70],'o',label='$\\\\lambda = %0.1e$' % ldom[70])\n",
    "plt.plot(rss[80],reg[80],'o',label='$\\\\lambda = %0.1e$' % ldom[80])\n",
    "plt.plot(bestRSS, bestReg, '*k', label='$\\\\lambda^*$ = %0.1e' % bestLambda, markersize=10)\n",
    "plt.xlabel('Residual Norm $||A \\\\beta - y||_2^2$', fontsize=16)\n",
    "plt.ylabel('Solution Norm $||\\\\beta||_2^2$', fontsize=16)\n",
    "plt.legend(loc=3, fontsize=16)\n",
    "plt.title('L-Curve', fontsize=18)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.loglog(ldom, mse)\n",
    "plt.axvline(bestLambda, color='k', linestyle=':', label='$\\\\lambda^*$')\n",
    "plt.xlabel('Regularization Term $\\\\lambda$', fontsize=16)\n",
    "plt.ylabel('MSE($\\\\beta$,$\\hat{\\\\beta}$)', fontsize=16)\n",
    "plt.title('True MSE vs. Regularization', fontsize=18)\n",
    "plt.legend(loc=2, fontsize=16)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(d1,d2)\n",
    "plt.plot(d1[0],d2[0],'ok', label='$\\\\lambda \\\\rightarrow \\\\infty$')\n",
    "plt.plot(d1[-1],d2[-1],'or', label='$\\\\lambda \\\\rightarrow 0$')\n",
    "plt.xlabel('$\\\\hat{\\\\beta}_{15}$', fontsize=16)\n",
    "plt.ylabel('$\\\\hat{\\\\beta}_{30}$', fontsize=16)\n",
    "plt.title('Coefficient Path', fontsize=18)\n",
    "plt.legend(loc=1, fontsize=16)\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of the L-Curve is to find the \"balance\" point between minimizing the solution norm and minimizing the residual norm, i.e. the bend of the curve. Another approach is simply to use cross validation, but it gives you an idea of the effect of the regularization. As $\\lambda$ increases, the $||\\beta||_2 \\rightarrow 0$, hence the term shrinkage. But, we can decrease our bias by going in the direction $\\lambda \\rightarrow 0$. Here, we see that the solution norm rises, but one can minimize the residual error. However, in this case, we will probably fail on generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso\n",
    "----\n",
    "Another approach that we can use is the Lasso for sparse regression. In this case, rather than penalizing the energy of the coefficients, $||\\boldsymbol{\\beta}||_2^2$, instead we penalize the $\\ell_1$ norm, \n",
    "$$||\\boldsymbol{\\beta}||_1 = \\sum_i |\\beta_i|.$$\n",
    "When we use this prior we are promiting the **sparsity** of the parameters. This should place hard-zeros on \"insignificant\" features, and assign most of the model weight to a few features. \n",
    "\n",
    "The estimator for the Lasso is written similarily as\n",
    "$$\\hat{\\boldsymbol{\\beta}} = {\\rm arg~min}_{\\boldsymbol{\\beta}} \n",
    "\\quad ||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} ||_2^2 + \\lambda ||\\boldsymbol{\\beta}||_1 $$.\n",
    "Now, how do we attempt to find this estimator? Unfortunately, there isn't a closed form solution for this estimator. Instead we need to write some convex optimization (or fixed-point iteration) to recover the estimator. Writing this estimator is a bit beyond the scope of this notebook. So, instead, we will use the Lasso implemented in Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "#--- Initialize ---#\n",
    "lasso = linear_model.Lasso(alpha=0.01)\n",
    "\n",
    "#--- Fit to the Data ---#\n",
    "lasso.fit(X,y)\n",
    "\n",
    "#--- Visualization ---#\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.stem(lasso.coef_, label='Lasso $\\\\hat{\\\\beta}$')\n",
    "plt.plot(np.argwhere(betaModel),betaModel[betaModel != 0],'kx', label='True $\\\\beta$')\n",
    "plt.axis([0, p-1, -7, 7])\n",
    "plt.xlabel('Features', fontsize=16)\n",
    "plt.ylabel('Coefficient Value', fontsize=16)\n",
    "plt.legend(loc=4, fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so, we can now see that we can recover these two singificant features quite easily using the $\\ell_1$-regularization ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldom = np.logspace(-5,2,100)\n",
    "\n",
    "rss = np.zeros(len(ldom))\n",
    "reg = np.zeros(len(ldom))\n",
    "mse = np.zeros(len(ldom))\n",
    "d1  = np.zeros(len(ldom))\n",
    "d2  = np.zeros(len(ldom))\n",
    "\n",
    "for (i,l) in enumerate(ldom):\n",
    "    fit = linear_model.Lasso(alpha=l).fit(X,y)\n",
    "    rss[i] = np.mean(np.power(y - fit.predict(X),2))\n",
    "    reg[i] = np.sum(np.abs(fit.coef_))\n",
    "    mse[i] = np.mean(np.power(betaModel - fit.coef_,2))\n",
    "    d1[i] = fit.coef_[15]\n",
    "    d2[i] = fit.coef_[30]\n",
    "    \n",
    "bestLambda = ldom[np.argmin(mse)] \n",
    "bestRSS = rss[np.argmin(mse)]\n",
    "bestReg = reg[np.argmin(mse)]\n",
    "    \n",
    "#--- Visualize ---#\n",
    "plt.figure(figsize=(15,5))    \n",
    "plt.subplot(131)\n",
    "plt.loglog(rss,reg)\n",
    "plt.plot(rss[10],reg[10],'o',label='$\\\\lambda = %0.1e$' % ldom[10])\n",
    "plt.plot(rss[20],reg[20],'o',label='$\\\\lambda = %0.1e$' % ldom[20])\n",
    "plt.plot(rss[30],reg[30],'o',label='$\\\\lambda = %0.1e$' % ldom[30])\n",
    "plt.plot(rss[40],reg[40],'o',label='$\\\\lambda = %0.1e$' % ldom[40])\n",
    "plt.plot(bestRSS, bestReg, '*k', label='$\\\\lambda^*$ = %0.1e' % bestLambda, markersize=10)\n",
    "plt.xlabel('Residual Norm $||A \\\\beta - y||_2^2$', fontsize=16)\n",
    "plt.ylabel('Solution Norm $||\\\\beta||_1$', fontsize=16)\n",
    "plt.legend(loc=3, fontsize=16)\n",
    "plt.title('L-Curve', fontsize=18)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.loglog(ldom, mse)\n",
    "plt.axvline(bestLambda, color='k', linestyle=':', label='$\\\\lambda^*$')\n",
    "plt.xlabel('Regularization Term $\\\\lambda$', fontsize=16)\n",
    "plt.ylabel('MSE($\\\\beta$,$\\hat{\\\\beta}$)', fontsize=16)\n",
    "plt.title('True MSE vs. Regularization', fontsize=18)\n",
    "plt.legend(loc=2, fontsize=16)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(d1,d2)\n",
    "plt.plot(d1[0],d2[0],'ok', label='$\\\\lambda \\\\rightarrow \\\\infty$')\n",
    "plt.plot(d1[-1],d2[-1],'or', label='$\\\\lambda \\\\rightarrow 0$')\n",
    "plt.xlabel('$\\\\hat{\\\\beta}_{15}$', fontsize=16)\n",
    "plt.ylabel('$\\\\hat{\\\\beta}_{30}$', fontsize=16)\n",
    "plt.title('Coefficient Path', fontsize=18)\n",
    "plt.legend(loc=1, fontsize=16)\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
