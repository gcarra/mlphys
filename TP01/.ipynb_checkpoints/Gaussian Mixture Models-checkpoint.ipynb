{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"font.size\"] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning: GMMs for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us think about the generative model we used in the last notebook. For each class we implemented the following procedure\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{align}\n",
    "{\\vec m}_k &\\sim \\mathcal{N} (0, I_2), &\\quad& k = 1, \\dots, K \\\\\n",
    "j_i &\\sim \\operatorname{Uniform} ([K]), &\\quad& i = 1, \\dots, N \\\\\n",
    "{\\vec x}_i &\\sim \\mathcal{N} ({\\vec m}_{j_i}, \\sigma^2 I_2), &\\quad& i = 1, \\dots, N\n",
    "\\end{align}\n",
    "\\right.\n",
    "$$\n",
    "Assuming the centroids are known, the likelihood for this model can be written as\n",
    "$$\n",
    "P({\\vec x}_i | \\{ {\\vec m}_k \\}, {\\vec x} \\in C_\\ell) = \\frac{1}{K} \\sum_{k = 1}^K \\mathcal{N} ({\\vec x}_i ; {\\vec m_k}, \\sigma^2 I_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Bayes theorem, we can see that the probability of $\\vec x_i$ belonging to class $C_\\ell$ is just $P({\\vec x} \\in C_\\ell | {\\vec x}_i, \\{{\\vec m}_k\\})) \\propto P({\\vec x}_i | \\{ \\vec{m}_k \\}, {\\vec x} \\in C_\\ell) \\, P_0 ({\\vec x} \\in C_\\ell)$. If moreover we assume a uniform prior, determining whether a given sample belongs to class 0 or 1 is just a matter of computing the two likelihoods and seeing which one is larger.\n",
    "\n",
    "Let us do that and compare to the k-NN estimator we studied earlier. First we generate samples as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Samples 10 centroids for each class from two different bivariate Normal distributions\n",
    "centroids_per_class = 10\n",
    "\n",
    "class0_centroids = [1, 0] + np.random.randn(centroids_per_class, 2)\n",
    "class1_centroids = [0, 1] + np.random.randn(centroids_per_class, 2)\n",
    "\n",
    "# Given the centroids, sample data\n",
    "samples_per_class = 100\n",
    "\n",
    "class0_labels = np.random.randint(10, size = samples_per_class)\n",
    "class0_samples = class0_centroids[class0_labels, :] + np.sqrt(1. / 5) * np.random.randn(samples_per_class, 2)\n",
    "\n",
    "class1_labels = np.random.randint(10, size = samples_per_class)\n",
    "class1_samples = class1_centroids[class1_labels, :] + np.sqrt(1. / 5) * np.random.randn(samples_per_class, 2)\n",
    "\n",
    "# Put data together\n",
    "X = np.vstack((class0_samples, class1_samples))\n",
    "y = np.hstack((np.zeros(samples_per_class), np.ones(samples_per_class)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train our k-NN classifier. Now that we know about scikit-learn we can to that very easily :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set up the grid search\n",
    "parameters = [{'n_neighbors': np.arange(1, 20)}]\n",
    "clf = GridSearchCV(neighbors.KNeighborsClassifier(n_neighbors = 1), parameters)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Print results\n",
    "print(clf.best_params_)\n",
    "print(\"training error = %g\" % np.mean(y != clf.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can also generate a test set and compute the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Centroids are already given; just sample the data\n",
    "testsamples_per_class = 10000\n",
    "\n",
    "class0_testlabels = np.random.randint(10, size = testsamples_per_class)\n",
    "class0_testsamples = class0_centroids[class0_testlabels, :] + np.sqrt(1. / 5) * np.random.randn(testsamples_per_class, 2)\n",
    "\n",
    "class1_testlabels = np.random.randint(10, size = testsamples_per_class)\n",
    "class1_testsamples = class1_centroids[class1_testlabels, :] + np.sqrt(1. / 5) * np.random.randn(testsamples_per_class, 2)\n",
    "\n",
    "# Put things together\n",
    "X_test = np.vstack((class0_testsamples, class1_testsamples))\n",
    "y_test = np.hstack((np.zeros(testsamples_per_class), np.ones(testsamples_per_class)))\n",
    "\n",
    "# Print test error\n",
    "print(\"test error = %g\" % np.mean(y_test != clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about our Bayes-optimal classifier? Let us implement a function to compute the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_posterior(x, centroids, var):\n",
    "    return np.sum(np.exp(-.5 * np.sum((x - centroids) ** 2, 1) / var) / np.sqrt(2 * np.pi * var))\n",
    "\n",
    "def bayes(X, m0, m1, var):\n",
    "    estimate = np.zeros(len(X))\n",
    "    for i in range(len(X)):\n",
    "        estimate[i] =\n",
    "    return estimate\n",
    "\n",
    "y_hat = bayes(X_test, class0_centroids, class1_centroids, 1./5)\n",
    "print(\"error on test set: %g\" % np.mean(y_test != y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment the following line to get the solution\n",
    "#%load gmm1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually got very close with the $k$-NN estimate, while not using any information at all from the model! The Bayes-optimal classifier gives us the smallest possible error, *given that* the generative model is known. Of course, that is most often not the case when one is working with real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's plot the posterior for each class to see how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate grid\n",
    "xx, yy = np.meshgrid(np.linspace(-3, 3, 101), np.linspace(-3, 3, 101))\n",
    "xs = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# Compute posterior at each point of the grid using list comprehensions\n",
    "ys0 = np.array([compute_posterior(x, class0_centroids, 1./5) for x in xs])\n",
    "ys1 = np.array([compute_posterior(x, class1_centroids, 1./5) for x in xs])\n",
    "\n",
    "# Do plot\n",
    "fig, axs = plt.subplots(1, 2, figsize = (12, 6))\n",
    "axs[0].pcolormesh(xx, yy, ys0.reshape((101, 101)), cmap = \"jet\")\n",
    "axs[0].set_title(\"class 0\")\n",
    "axs[1].pcolormesh(xx, yy, ys1.reshape((101, 101)), cmap = \"jet\")\n",
    "axs[1].set_title(\"class 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Side exercise**: implement the Bayes classifier for the case where the centroids are not known (you will need the EM algorithm introduced by Florent yesterday). If you don't want to code everything by hand, take a look at what's available on scikit-learn.\n",
    "Were you able to recover the centroids accurately?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning: GMMs for clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As discussed in Florent's lecture yesterday, GMMs are also very useful in the case where the labels are not known and we want to cluster the samples into different groups. Let us try to cluster the digits on MNIST to see if we are able to get a good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "X = mnist.data\n",
    "y = mnist.target\n",
    "\n",
    "# Let's work only with 0's, 1's and 2's\n",
    "mask = np.logical_or(y == 0, np.logical_or(y == 1, y == 2))\n",
    "X = X[mask, :]\n",
    "y = y[mask]\n",
    "\n",
    "n_samples, n_features = np.shape(X)\n",
    "print(n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's still a lot of samples, so let us reduce the size of our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_train_samples = 1000\n",
    "train_samples = np.random.randint(n_samples, size = n_train_samples)\n",
    "X_train, y_train = X[train_samples, :], y[train_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now next is quite simple to run scikit-learn's implementation of GMMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "estimator = GaussianMixture(n_components = 3)\n",
    "estimator.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat0 = estimator.predict(X_train)\n",
    "\n",
    "print(y_train)\n",
    "print(y_hat0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh-oh, these look like, completely different. What's going on? Well, of course, our model does not know the correct labels, so it cannot predict them accordingly! We need to look for permutations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "for p in permutations(range(3)):\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: compute error for each of the label permutations, and determine the correct one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error_min = 1.0\n",
    "for p in permutations(range(3)):\n",
    "    # Permute labels following permutation p\n",
    "    \n",
    "    # Compute error; if smaller than min. so far, store permutation\n",
    "\n",
    "    print(\"permutation: %s, error: %g\" % (p, error))\n",
    "    \n",
    "# Permute labels following permutation that gives minimum error\n",
    "y_hat = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment the following line to get the solution to the exercise\n",
    "#%load gmm2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can play a bit with the actual and the estimated number of classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
