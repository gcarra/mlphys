{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lighthouse problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"font.size\"] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Scheme](lighthouse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the following problem (extracted from D. Sivia's book, \"Data Analysis - A Bayesian Tutorial\"):\n",
    "\n",
    "> A lighthouse is somewhere off a piece of straight coastline at a position $\\alpha$ along the shore and a distance $\\beta$ out at sea. It emits a series of short highly collimated flashes at random intervals and hence at random azimuths. These pulses are intercepted on the coast by photo-detectors that record only the fact that a flash has occurred, but not the angle from which it came. $N$ flashes have been recorded so far at positions $\\{x_k\\}$. Where is the lighthouse?\n",
    "\n",
    "For simplicity, we will assume $\\beta$ is known, so that the only parameter we need to estimate is $\\alpha$. Let us start by writing the likelihood for this problem; since the flashes are thrown at random azimuths, we know that\n",
    "\n",
    "$$P(\\theta_k | \\alpha, \\beta) = \\frac{1}{\\pi}$$\n",
    "\n",
    "Moreover\n",
    "\n",
    "$$\\beta \\tanh(\\theta_k) = x_k - \\alpha$$\n",
    "\n",
    "and by changing variables we get\n",
    "\n",
    "$$P(x_k | \\alpha, \\beta) = \\frac{\\beta}{\\pi \\big[ \\beta^2 + (x_k - \\alpha)^2 \\big]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let us compute and plot the likelihood\n",
    "def likelihood(x, alpha, beta):\n",
    "    return beta / (np.pi * (beta ** 2 + (x - alpha) ** 2))\n",
    "\n",
    "# Parameters\n",
    "alpha = 5.0\n",
    "beta = 10.0\n",
    "\n",
    "x = np.linspace(-90, 90, 1001)\n",
    "plt.plot(x, likelihood(x, alpha, beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood is known as a Cauchy or Lorentz distribution. Let us sample from it so that we can have some synthetic data to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we generate some synthetic data\n",
    "alpha = 30.0\n",
    "beta = 10.0\n",
    "\n",
    "from scipy.stats import cauchy\n",
    "samples = cauchy.rvs(loc = alpha, scale = beta, size = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to write the posterior we use Bayes theorem\n",
    "\n",
    "$$P(\\alpha | \\{x_k\\}, \\beta) \\propto \\prod_{k = 1}^N P(\\{x_k\\} | \\alpha, \\beta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Computes the (unnormalized) posterior for a given set of samples\n",
    "def posterior(x, alpha, beta):\n",
    "    post = np.ones(len(alpha))\n",
    "    for x_k in x:\n",
    "        post *= likelihood(x_k, alpha, beta)\n",
    "        post /= np.sum(post)\n",
    "    return post\n",
    "\n",
    "def plot_posterior(n_samples):\n",
    "    alphas = np.linspace(0, 60, 1001)\n",
    "    plt.plot(alphas, posterior(samples[:n_samples], alphas, beta))\n",
    "    plt.axvline(np.mean(samples[:n_samples]), c = \"r\", lw = 2)\n",
    "    \n",
    "plot_posterior(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: do a set w/ 4 subplots for different values of $N = 2, 5, 20, 100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.tight_layout()\n",
    "\n",
    "alphas = np.linspace(0, 60, 1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load lighthouse1.py\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.tight_layout()\n",
    "\n",
    "alphas = np.linspace(0, 60, 1001)\n",
    "\n",
    "axs[0,0].plot(alphas, posterior(samples[:2], alphas, beta))\n",
    "axs[0,0].axvline(np.mean(samples[:2]), c = \"r\", lw = 2)\n",
    "axs[0,0].set_title(r\"$N = 2$\")\n",
    "axs[0,1].plot(alphas, posterior(samples[:5], alphas, beta))\n",
    "axs[0,1].axvline(np.mean(samples[:5]), c = \"r\", lw = 2)\n",
    "axs[0,1].set_title(r\"$N = 5$\")\n",
    "axs[1,0].plot(alphas, posterior(samples[:20], alphas, beta))\n",
    "axs[1,0].axvline(np.mean(samples[:20]), c = \"r\", lw = 2)\n",
    "axs[1,0].set_title(r\"$N = 20$\")\n",
    "axs[1,1].plot(alphas, posterior(samples[:100], alphas, beta))\n",
    "axs[1,1].axvline(np.mean(samples[:100]), c = \"r\", lw = 2)\n",
    "axs[1,1].set_title(r\"$N = 100$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the mean does not coincide with the mode of the posterior! Why is that? Will they coincide in the $N \\to \\infty$ limit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us compute the value of $\\alpha$ that actually maximizes the posterior (and the likelihood, since our prior here is uniform). The log-likelihood reads\n",
    "\n",
    "$$\\mathcal{L} (\\alpha) = \\sum_k \\log P(x_k | \\alpha, \\beta) = -\\sum_k \\log[\\beta^2 + (x_k - \\alpha)^2)] + {\\rm cst.}$$\n",
    "\n",
    "so that the maximum is obtained at\n",
    "\n",
    "$$2 \\sum_k \\frac{x_k - \\alpha^\\ast}{\\beta^2 + (x_k - \\alpha^\\ast)^2} = 0$$\n",
    "\n",
    "Let us try to solve this numerically for different values of $N$.\n",
    "\n",
    "**Exercise**: plot the ML estimate of $\\alpha$ for $N$ between 10 and 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import brentq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load lighthouse2.py\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "def f(alpha, x):\n",
    "    return 2 * np.sum((x - alpha) / (beta ** 2 + (x - alpha) ** 2))\n",
    "\n",
    "ns = range(10,1000)\n",
    "alphas = [brentq(f, -90, 90, args = samples[:n_samples]) for n_samples in ns]\n",
    "plt.plot(ns, alphas)\n",
    "plt.axhline(30, color = \"r\", lw = 2)\n",
    "plt.xlabel(r\"$N$\")\n",
    "plt.ylabel(r\"$\\alpha$\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
