{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define and open the local OAuth token configuration file\n",
    "oauth_token_file_name = 'twitterOAuth.cfg'\n",
    "oauth_token_file = open(oauth_token_file_name,'r')\n",
    "\n",
    "# Read tokens from local file. This file consists of four lines, \n",
    "# ended in a newline. Each of these lines contains the in the order\n",
    "# specified below.\n",
    "consumer_key = oauth_token_file.readline()[:-1] # Strip '\\n'\n",
    "consumer_secret = oauth_token_file.readline()[:-1]\n",
    "access_token = oauth_token_file.readline()[:-1]\n",
    "access_token_secret = oauth_token_file.readline()[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configure and intialize the OAuth class\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Initialize Twitter API access\n",
    "api = tweepy.API(auth,wait_on_rate_limit = True, wait_on_rate_limit_notify = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets get information about a user\n",
    "target_user_name = 'RealDonaldTrump'\n",
    "target_user = api.get_user(target_user_name)\n",
    "\n",
    "# Some information\n",
    "print('@%s' % target_user_name)\n",
    "print('----------------------')\n",
    "print('ID : %s' % target_user.id)\n",
    "print('Followers: %s' % target_user.followers_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the Latest 3,200 Tweets\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open database CSV file\n",
    "tweet_database_file_name = 'donald_dump_fresh.dat'\n",
    "tweet_database_file = open(tweet_database_file_name, 'w')\n",
    "# Write the header\n",
    "tweet_database_file.write('id\\tsource\\tcreated_at\\ttext\\tfavorite_count\\tretweet_count\\tcoords\\n')\n",
    "\n",
    "# Define a function which will store the necessary details of the tweets\n",
    "def store_tweet(file, status, delim = '\\t'):\n",
    "    file.write('%d%s' % (status.id,delim))\n",
    "    file.write('\"%s\"%s' % (status.source,delim))\n",
    "    # Write time\n",
    "    timestamp = status.created_at.timestamp()\n",
    "    file.write('%s%s' % (status.created_at.strftime('%a %b %d %H:%M:%S %z %Y'),delim))\n",
    "    # Process text ...?    \n",
    "    text = status.text.replace('\\n',' ')\n",
    "    file.write('\"%s\"%s' % (text,delim))\n",
    "    file.write('%d%s' % (status.favorite_count,delim))\n",
    "    file.write('%d%s' % (status.retweet_count,delim))\n",
    "    # Is there a coordinate ? \n",
    "    coord = 'NA' if (status.coordinates == None) else '\"%s\"' % status.coordinates\n",
    "    file.write('%s' % coord)\n",
    "    file.write('\\n')\n",
    "\n",
    "# Process and write tweets\n",
    "maximum_tweets = 100\n",
    "\n",
    "for status in tweepy.Cursor(api.user_timeline, id = target_user.id).items(maximum_tweets):\n",
    "    store_tweet(tweet_database_file,status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recover Older Trump Tweets\n",
    "----\n",
    "Because of access restrictions we cannot get to tweets older than the latest 3,200 via the  Twitter API. Thanks to some hard work by other users, we can access a less-frequently updated but futher-reaching database of Trump tweets. \n",
    "\n",
    "This dataset is updated irregularily, so it could be some days inbetween "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "old_donald_dump_url = 'https://raw.githubusercontent.com/bpb27/political_twitter_archive/master/realdonaldtrump/realdonaldtrump.csv'\n",
    "\n",
    "# Retrieve the list of tweet ids\n",
    "old_tweets = pd.read_csv(old_donald_dump_url)\n",
    "old_tweet_ids = old_tweets['id_str']\n",
    "\n",
    "# Create a DataFrame for the Fresh Tweets we aleady have\n",
    "tweet_database_file_name = 'donald_dump_fresh.dat'\n",
    "new_tweets = pd.read_table(tweet_database_file_name, delimiter='\\t', header=0)\n",
    "\n",
    "# Find all tweets older than the last one we viewed\n",
    "oldest_fresh_tweet_id = new_tweets['id'][len(new_tweets) - 1]\n",
    "\n",
    "# Not a \"yuuuuuge\" data problem...grab the sublist for older tweet ids\n",
    "old_tweets_to_keep = [True if x < oldest_fresh_tweet_id else False for x in old_tweet_ids]\n",
    "old_tweets = old_tweets.iloc[old_tweets_to_keep]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that Twitter limits us to 15 API calls every 15 minutes, or 1 API call per minute. Considering the very large number of tweets Trump has made since 2009, this means that it is impractical to call each of these tweets individually.\n",
    "\n",
    "Instead, lets concatenate the fields that we can into the existing dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modify the columns of the old tweets to match the fresh tweets\n",
    "from numpy import nan\n",
    "n = len(old_tweets)\n",
    "old_tweets['coords'] = [nan for i in range(0,n)]\n",
    "old_tweets.sort_values('id_str',ascending=False, inplace=True)\n",
    "old_tweets['id'] = old_tweets['id_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Brute-force merge\n",
    "column_choice = ['id','created_at','text','retweet_count','favorite_count','source']\n",
    "all_tweets = pd.DataFrame(columns = column_choice)\n",
    "all_tweets = all_tweets.append(new_tweets.ix[:,column_choice], ignore_index = True)\n",
    "all_tweets = all_tweets.append(old_tweets.ix[:,column_choice], ignore_index = True)\n",
    "\n",
    "# Ensure integer fields\n",
    "all_tweets[['retweet_count','favorite_count']] = all_tweets[['retweet_count','favorite_count']].apply(pd.to_numeric,downcast='integer')\n",
    "# Ensure strings when needed\n",
    "all_tweets['id'] = all_tweets['id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write to disk\n",
    "import csv\n",
    "final_database_file = 'donald_dump.dat'\n",
    "all_tweets.to_csv(final_database_file,sep='\\t',header=True,quoting=csv.QUOTE_NONNUMERIC,float_format='%.1f',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(all_tweets['id'][3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = pd.DataFrame(np.random.randn(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'123456789123456784.0'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "'%.1f' % 123456789123456789.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
